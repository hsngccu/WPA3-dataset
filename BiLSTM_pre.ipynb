{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "from multiprocessing import Pool\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from imblearn.over_sampling import KMeansSMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "wpa3_dataset_class_list = ['BF', 'Deauth', 'SAE_CV', 'UGD', 'SAE_AF', 'TSC', 'DG', 'KRACK_DG']\n",
    "WIDS_class_list = ['benign', 'BF', 'Deauth', 'SAE_CV', 'UGD', 'SAE_AF', 'TSC', 'DG', 'KRACK_DG']\n",
    "use_feat = {\n",
    "    \"frame.len\": \"Int64\",\n",
    "    \"radiotap.length\": \"Int64\",\n",
    "    \"radiotap.dbm_antsignal\": \"Int64\",\n",
    "    \"wlan.duration\": \"Int64\",\n",
    "    \"radiotap.present.tsft\": \"Int64\",\n",
    "    \"radiotap.channel.freq\": \"Int64\",\n",
    "    \"radiotap.channel.flags.cck\": \"Int64\",\n",
    "    \"radiotap.channel.flags.ofdm\": \"Int64\",\n",
    "    \"wlan.fc.type\": \"Int64\",\n",
    "    \"wlan.fc.subtype\": \"Int64\",\n",
    "    \"wlan.fc.ds\": str,\n",
    "    \"wlan.fc.frag\": \"Int64\",\n",
    "    \"wlan.fc.retry\": \"Int64\",\n",
    "    \"wlan.fc.pwrmgt\": \"Int64\",\n",
    "    \"wlan.fc.moredata\": \"Int64\",\n",
    "    \"wlan.fc.protected\": \"Int64\",\n",
    "    \"wlan.fixed.status_code\": \"Int64\",\n",
    "    \"wlan.fixed.auth_seq\": str,\n",
    "    \"Label\": str,\n",
    "}\n",
    "\n",
    "root_path = f\"/home/sun10/WIDS\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 產生 npy file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GENERATE_FEAT_NPY():\n",
    "    def __init__(\n",
    "        self, path: list[str], att: str, use_feat: dict\n",
    "    ):\n",
    "        self.path = path\n",
    "        self.att = att\n",
    "        self.CPU_CORE = int(30)\n",
    "        self.use_feat = use_feat\n",
    "        self.directory = f\"{root_path}/data/{self.att}\"\n",
    "\n",
    "    def _clean_row_data(self, row):\n",
    "        keys = self.use_feat.keys()\n",
    "        if \"wlan.fixed.auth_seq\" in keys:\n",
    "            row[\"wlan.fixed.auth_seq\"] = int(\n",
    "                str(row[\"wlan.fixed.auth_seq\"]), 16)\n",
    "        if \"wlan.fc.ds\" in keys:\n",
    "            row[\"wlan.fc.ds\"] = int(str(row[\"wlan.fc.ds\"]), 16)\n",
    "\n",
    "        if \"radiotap.rxflags\" in keys:\n",
    "            row[\"radiotap.rxflags\"] = int(str(row[\"radiotap.rxflags\"]), 16)\n",
    "        if \"wlan.tag.length\" in keys:\n",
    "            if str(row[\"wlan.tag.length\"]).find(\" \") != -1:\n",
    "                row[\"wlan.tag.length\"] = float(\n",
    "                    str(row[\"wlan.tag.length\"]).split(' ')[0])\n",
    "            else:\n",
    "                row[\"wlan.tag.length\"] = float(row[\"wlan.tag.length\"])\n",
    "        return row\n",
    "\n",
    "    def _deal_file(self, path: str):\n",
    "        selected = self.use_feat\n",
    "        selected_columns = list(selected.keys())\n",
    "        # 從CSV文件中讀取數據\n",
    "        df = pd.read_csv(path, usecols=selected_columns,\n",
    "                         dtype=selected)  # , dtype=selected\n",
    "\n",
    "        df.fillna(0, inplace=True)\n",
    "        df = df.apply(self._clean_row_data, axis=1)\n",
    "\n",
    "        df.fillna(0, inplace=True)\n",
    "        b_pkts = df[df[\"Label\"] == \"Normal\"].copy()\n",
    "        m_pkts = df[df[\"Label\"] != \"Normal\"].copy()\n",
    "\n",
    "        b_pkts.drop([\"Label\"], axis=1, inplace=True)\n",
    "        m_pkts.drop([\"Label\"], axis=1, inplace=True)\n",
    "        b_data = b_pkts.values.tolist()\n",
    "        m_data = m_pkts.values.tolist()\n",
    "\n",
    "        return b_data, m_data\n",
    "\n",
    "    def _save_np_files(self, b_flows: list, m_flows: list):\n",
    "        # list to numpy array\n",
    "        m_flows = np.asarray(m_flows)\n",
    "        b_flows = np.asarray(b_flows)\n",
    "\n",
    "        # make directories\n",
    "        directory = self.directory\n",
    "\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "\n",
    "        # delete old file\n",
    "        if os.path.isfile(f\"{directory}/malicious_t.npy\"):\n",
    "            os.remove(f\"{directory}/malicious_t.npy\")\n",
    "        if os.path.isfile(f\"{directory}/benign_t.npy\"):\n",
    "            os.remove(f\"{directory}/benign_t.npy\")\n",
    "\n",
    "        print(f'{\"*\" * 11} Number of {self.att} class {\"*\" * 11}')\n",
    "        # save new file\n",
    "        np.save(f\"{directory}/malicious_t\", m_flows)\n",
    "        print(f\"* malicious has {len(m_flows)} flows\")\n",
    "\n",
    "        np.save(f\"{directory}/benign_t\", b_flows)\n",
    "        print(f\"* benign    has {len(b_flows)} flows\")\n",
    "\n",
    "    def run(self):\n",
    "        b_flows = list([])\n",
    "        m_flows = list([])\n",
    "\n",
    "        chunk_size = self.CPU_CORE\n",
    "        num_chunks = len(self.path) // chunk_size\n",
    "        remainder = len(self.path) % chunk_size\n",
    "        # 對每個完整的塊應用 deal_file 函數\n",
    "        for i in range(num_chunks):\n",
    "            chunk = self.path[i * chunk_size: (i + 1) * chunk_size]\n",
    "            pool = Pool(chunk_size)  # 設定處理程序數量\n",
    "            results = pool.map_async(self._deal_file, chunk)\n",
    "            pool_result = results.get()\n",
    "            for r in pool_result:\n",
    "                tmp_b_flows, tmp_m_flows = r\n",
    "                b_flows.extend(tmp_b_flows)\n",
    "                m_flows.extend(tmp_m_flows)\n",
    "                del tmp_b_flows, tmp_m_flows\n",
    "\n",
    "        # 如果有剩餘的資料，也將其作為一個單獨的塊處理\n",
    "        if remainder > 0:\n",
    "            last_chunk = self.path[num_chunks * chunk_size:]\n",
    "            pool = Pool(len(last_chunk))  # 設定處理程序數量\n",
    "            results = pool.map_async(self._deal_file, last_chunk)\n",
    "            pool_result = results.get()\n",
    "            for r in pool_result:\n",
    "                tmp_b_flows, tmp_m_flows = r\n",
    "                b_flows.extend(tmp_b_flows)\n",
    "                m_flows.extend(tmp_m_flows)\n",
    "                del tmp_b_flows, tmp_m_flows\n",
    "\n",
    "        self._save_np_files(b_flows, m_flows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*********** Number of BF class ***********\n",
      "* malicious has 54083 flows\n",
      "* benign    has 145240 flows\n",
      "*********** Number of Deauth class ***********\n",
      "* malicious has 5507 flows\n",
      "* benign    has 116799 flows\n",
      "*********** Number of SAE_CV class ***********\n",
      "* malicious has 1210 flows\n",
      "* benign    has 116722 flows\n",
      "*********** Number of UGD class ***********\n",
      "* malicious has 2344 flows\n",
      "* benign    has 157855 flows\n",
      "*********** Number of SAE_AF class ***********\n",
      "* malicious has 90904 flows\n",
      "* benign    has 155751 flows\n",
      "*********** Number of TSC class ***********\n",
      "* malicious has 819 flows\n",
      "* benign    has 149259 flows\n",
      "*********** Number of DG class ***********\n",
      "* malicious has 11296 flows\n",
      "* benign    has 127643 flows\n",
      "*********** Number of KRACK_DG class ***********\n",
      "* malicious has 7330 flows\n",
      "* benign    has 160763 flows\n"
     ]
    }
   ],
   "source": [
    "flag = 1\n",
    "for att in wpa3_dataset_class_list:\n",
    "    csv_path = f\"{root_path}/The_WPA3_Dataset_csv/{flag}. {att}/{flag}. {att}_*.csv\"\n",
    "    paths = glob.glob(csv_path)\n",
    "    npy = GENERATE_FEAT_NPY(paths, att, use_feat)\n",
    "    npy.run()\n",
    "    del npy\n",
    "    flag += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 切割 training and testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SPLIT_SET():\n",
    "    def __init__(self, data_path: str, classes: list):\n",
    "        self.data_path = data_path\n",
    "        self.classes = classes\n",
    "        self.save_to = f\"{root_path}/data\"\n",
    "\n",
    "    def save_np(self, path, data):\n",
    "        if os.path.isfile(path):\n",
    "            os.remove(path)\n",
    "        np.save(path, data)\n",
    "    \n",
    "    def concat(self, old_data, new_data):\n",
    "        if old_data is None and new_data.shape[0] != 0:\n",
    "            concat = new_data\n",
    "        else:\n",
    "            try:\n",
    "                concat = np.concatenate((old_data, new_data))\n",
    "            except ValueError:\n",
    "                concat = old_data\n",
    "        return concat\n",
    "\n",
    "    def trainTestSplit(self, data, proportion=0.2):\n",
    "        \"\"\"使用攻擊種類的惡意流量切割出train和test set\"\"\"\n",
    "        train_data, test_data = train_test_split(\n",
    "            data, test_size=proportion, random_state=42\n",
    "        )  # 分訓練/驗證\n",
    "        return train_data, test_data\n",
    "\n",
    "    def get_label(self, id, size):\n",
    "        return np.ones(size).astype(int) * (id)\n",
    "\n",
    "    def get_train_benign(self, data_path, ratio):\n",
    "        \"\"\"取得所有benign的npy檔案, 並切割出benign的train set\"\"\"\n",
    "\n",
    "        files = glob.glob(f\"{data_path}/*/benign_t.npy\")  # 取得所有benign的npy檔案\n",
    "        random_benign = None\n",
    "        for file in tqdm(files):\n",
    "            data = np.load(file)\n",
    "            total_size = data.shape[0]\n",
    "            sample_size = int(total_size * ratio)\n",
    "            # 使用numpy.random.choice来随机抽取数据\n",
    "            random_samples = np.random.choice(\n",
    "                total_size, size=sample_size, replace=False\n",
    "            )\n",
    "            random_data = data[random_samples]\n",
    "            # 將抽取數據後剩餘的數據儲存至untrain_benign.npy中\n",
    "            other_samples = np.setdiff1d(np.arange(total_size), random_samples)\n",
    "            other_path = file[: file.rfind(\"/\")]\n",
    "            self.save_np(f\"{other_path}/untrain_benign.npy\",\n",
    "                         data[other_samples])\n",
    "            random_benign = self.concat(random_benign, random_data)\n",
    "\n",
    "        return random_benign\n",
    "\n",
    "    def get_test_benign(self, data_path, ratio):\n",
    "        \"\"\"使用抽取train benign數據後的剩餘數據, 並切割出benign的test set\"\"\"\n",
    "\n",
    "        files = glob.glob(\n",
    "            f\"{data_path}/*/untrain_benign.npy\"\n",
    "        )  # 取得所有剩餘的benign的npy檔案\n",
    "        random_benign = None\n",
    "        for file in tqdm(files):\n",
    "            data = np.load(file)\n",
    "            total_size = data.shape[0]\n",
    "            sample_size = int(total_size * ratio)\n",
    "            # 使用numpy.random.choice来随机抽取数据\n",
    "            random_samples = np.random.choice(\n",
    "                total_size, size=sample_size, replace=False\n",
    "            )\n",
    "            random_data = data[random_samples]\n",
    "            # 將抽取數據後剩餘的數據儲存至untest_benign.npy中\n",
    "            other_samples = np.setdiff1d(np.arange(total_size), random_samples)\n",
    "            other_path = file[: file.rfind(\"/\")]\n",
    "            self.save_np(f\"{other_path}/untest_benign.npy\",\n",
    "                         data[other_samples])\n",
    "\n",
    "            random_benign = self.concat(random_benign, random_data)\n",
    "\n",
    "        return random_benign \n",
    "    \n",
    "    def run(self):\n",
    "        # 建立儲存資料夾路徑\n",
    "        # 建立儲存資料夾路徑\n",
    "        if not os.path.exists(f\"{self.save_to}/train/\"):\n",
    "            os.makedirs(f\"{self.save_to}/train/\")\n",
    "        if not os.path.exists(f\"{self.save_to}/test/\"):\n",
    "            os.makedirs(f\"{self.save_to}/test/\")\n",
    "\n",
    "        total_count = list([])\n",
    "        train_count = list([])\n",
    "        test_count = list([])\n",
    "\n",
    "\n",
    "        for index, c in enumerate(self.classes):\n",
    "            if c == \"benign\":\n",
    "                train_data = self.get_train_benign(self.data_path, 0.7)\n",
    "\n",
    "                self.save_np(f\"{self.save_to}/train/{c}.npy\", train_data)\n",
    "                train_size = train_data.shape[0]\n",
    "                del train_data\n",
    "                self.save_np(\n",
    "                    f\"{self.save_to}/train/{c}_label.npy\",\n",
    "                    self.get_label(index, train_size),\n",
    "                )\n",
    "                total_count.append(train_size)\n",
    "                train_count.append(train_size)\n",
    "\n",
    "                test_data = self.get_test_benign(self.data_path, 0.7)\n",
    "                test_size = test_data.shape[0]\n",
    "                self.save_np(f\"{self.save_to}/test/{c}.npy\", test_data)\n",
    "                del test_data\n",
    "                self.save_np(\n",
    "                    f\"{self.save_to}/test/{c}_label.npy\",\n",
    "                    self.get_label(index, test_size),\n",
    "                )\n",
    "                test_count.append(test_size)\n",
    "                continue\n",
    "\n",
    "            else:\n",
    "                npy_path = f\"{self.data_path}/{c}/malicious_t.npy\"\n",
    "                npy_data = np.load(npy_path)\n",
    "                try:\n",
    "                    c_num = npy_data.shape[0]\n",
    "                except AttributeError:\n",
    "                    print(f\" {c} No Data !\")\n",
    "                    c_num = 0\n",
    "\n",
    "                train_data, test_data = self.trainTestSplit(\n",
    "                    npy_data, proportion=0.2)\n",
    "                del npy_data  # 釋放記憶體\n",
    "\n",
    "                train_size = train_data.shape[0]\n",
    "                test_size = test_data.shape[0]\n",
    "\n",
    "                self.save_np(f\"{self.save_to}/train/{c}.npy\", train_data)\n",
    "                self.save_np(f\"{self.save_to}/test/{c}.npy\", test_data)\n",
    "                del train_data, test_data  # 釋放記憶體\n",
    "\n",
    "                # creat label .npy\n",
    "                self.save_np(\n",
    "                    f\"{self.save_to}/train/{c}_label.npy\",\n",
    "                    self.get_label(index, train_size),\n",
    "                )\n",
    "                self.save_np(\n",
    "                    f\"{self.save_to}/test/{c}_label.npy\",\n",
    "                    self.get_label(index, test_size),\n",
    "                )\n",
    "\n",
    "                total_count.append(c_num)\n",
    "                train_count.append(train_size)\n",
    "                test_count.append(test_size)\n",
    "\n",
    "        print(\"*\" * 15 + \" total size \" + \"*\" * 15)\n",
    "        for index, c in enumerate(self.classes):\n",
    "            print(f\"{c} total: {total_count[index]}\")\n",
    "        print(\"*\" * 15 + \" train size \" + \"*\" * 15)\n",
    "        for index, c in enumerate(self.classes):\n",
    "            print(f\"{c} total: {train_count[index]}\")\n",
    "        print(\"*\" * 15 + \" test size \" + \"*\" * 15)\n",
    "        for index, c in enumerate(self.classes):\n",
    "            print(f\"{c} total: {test_count[index]}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:00<00:00, 13.39it/s]\n",
      "100%|██████████| 8/8 [00:00<00:00, 57.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*************** total size ***************\n",
      "benign total: 791020\n",
      "BF total: 54083\n",
      "Deauth total: 5507\n",
      "SAE_CV total: 1210\n",
      "UGD total: 2344\n",
      "SAE_AF total: 90904\n",
      "TSC total: 819\n",
      "DG total: 11296\n",
      "KRACK_DG total: 7330\n",
      "*************** train size ***************\n",
      "benign total: 791020\n",
      "BF total: 43266\n",
      "Deauth total: 4405\n",
      "SAE_CV total: 968\n",
      "UGD total: 1875\n",
      "SAE_AF total: 72723\n",
      "TSC total: 655\n",
      "DG total: 9036\n",
      "KRACK_DG total: 5864\n",
      "*************** test size ***************\n",
      "benign total: 237305\n",
      "BF total: 10817\n",
      "Deauth total: 1102\n",
      "SAE_CV total: 242\n",
      "UGD total: 469\n",
      "SAE_AF total: 18181\n",
      "TSC total: 164\n",
      "DG total: 2260\n",
      "KRACK_DG total: 1466\n"
     ]
    }
   ],
   "source": [
    "split = SPLIT_SET(f\"{root_path}/data\", WIDS_class_list)\n",
    "split.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampling:\n",
    "    SEED = 42\n",
    "\n",
    "    def __init__(self, classes, data_path=None, test=False) -> None:\n",
    "        self.classes = classes\n",
    "        self.path = data_path\n",
    "        self.save_to = f\"{root_path}/sampling\"\n",
    "        self.test = test\n",
    "\n",
    "    def load_data(self, path=None):\n",
    "        if path == None:\n",
    "            path = self.path\n",
    "        # load data\n",
    "        x = []\n",
    "        y = []\n",
    "        for c in self.classes:\n",
    "            print(f\"Process Class: {c}\")\n",
    "            if not os.path.exists(f\"{path}\"):\n",
    "                os.makedirs(f\"{path}\")\n",
    "            data_path = f\"{path}/{c}.npy\"\n",
    "            label_path = f\"{path}/{c}_label.npy\"\n",
    "            x.extend(np.load(data_path))\n",
    "            y.extend(np.load(label_path))\n",
    "        # combine all the flow\n",
    "        x = np.asarray(x)\n",
    "        y = np.asarray(y)\n",
    "        return x, y\n",
    "\n",
    "    def get_oversampling(self, x, y, over_strategy, k, cluster_balance_threshold):\n",
    "        # output number of each class\n",
    "        print(\"*\" * 25 + \" Before Sampling \" + \"*\" * 25)\n",
    "        unique, counts = np.unique(y, return_counts=True)\n",
    "        print(f\"y: {dict(zip(unique, counts))}\")\n",
    "        # k-means smote oversampling\n",
    "        # over_strategy = {8: O}\n",
    "        oversample = KMeansSMOTE(\n",
    "            sampling_strategy=over_strategy,\n",
    "            random_state=self.SEED,\n",
    "            k_neighbors=k,\n",
    "            cluster_balance_threshold=cluster_balance_threshold,\n",
    "        )  # 0.0016\n",
    "        x, y = oversample.fit_resample(x, y)\n",
    "\n",
    "        # output number of each class\n",
    "        print(\"*\" * 25 + \" After OverSampling \" + \"*\" * 25)\n",
    "        unique, counts = np.unique(y, return_counts=True)\n",
    "        print(f\"y: {dict(zip(unique, counts))}\")\n",
    "        return x, y\n",
    "\n",
    "    def get_undersampling(self, x, y, under_strategy):\n",
    "        # output number of each class\n",
    "        print(\"*\" * 25 + \" Before Sampling \" + \"*\" * 25)\n",
    "        unique, counts = np.unique(y, return_counts=True)\n",
    "        print(f\"y: {dict(zip(unique, counts))}\")\n",
    "        # random undersampling\n",
    "        # under_strategy = {0: U, 1: U, 2: U, 3: U, 4: U, 5: U, 7: U, 8: U}\n",
    "        # under_strategy = {0: 360, 1: 360}\n",
    "        undersample = RandomUnderSampler(\n",
    "            sampling_strategy=under_strategy, random_state=self.SEED\n",
    "        )\n",
    "        x, y = undersample.fit_resample(x, y)\n",
    "\n",
    "        # output number of each class\n",
    "        print(\"*\" * 25 + \" After UnderSampling \" + \"*\" * 25)\n",
    "        unique, counts = np.unique(y, return_counts=True)\n",
    "        print(f\"y: {dict(zip(unique, counts))}\")\n",
    "        return x, y\n",
    "\n",
    "    def save_np(self, data, label):\n",
    "        if not self.test:\n",
    "            if not os.path.exists(f\"{self.save_to}\"):\n",
    "                os.makedirs(f\"{self.save_to}\")\n",
    "            # delete old file\n",
    "            if os.path.isfile(f\"{self.save_to}/train_data.npy\"):\n",
    "                os.remove(f\"{self.save_to}/train_data.npy\")\n",
    "            if os.path.isfile(f\"{self.save_to}/train_label.npy\"):\n",
    "                os.remove(f\"{self.save_to}/train_label.npy\")\n",
    "\n",
    "            if not os.path.isfile(f\"{self.save_to}/train_data.npy\"):\n",
    "                np.save(f\"{self.save_to}/train_data.npy\", data)\n",
    "            else:\n",
    "                print(f\"no delete old file\")\n",
    "            if not os.path.isfile(f\"{self.save_to}/train_label.npy\"):\n",
    "                np.save(f\"{self.save_to}/train_label.npy\", label)\n",
    "            else:\n",
    "                print(f\"no delete old file\")\n",
    "            print(f\"save to: {self.save_to}\")\n",
    "        elif self.test:\n",
    "            if not os.path.exists(f\"{self.save_to}\"):\n",
    "                os.makedirs(f\"{self.save_to}\")\n",
    "            # delete old file\n",
    "            if os.path.isfile(f\"{self.save_to}/test_data.npy\"):\n",
    "                os.remove(f\"{self.save_to}/test_data.npy\")\n",
    "            if os.path.isfile(f\"{self.save_to}/test_label.npy\"):\n",
    "                os.remove(f\"{self.save_to}/test_label.npy\")\n",
    "\n",
    "            if not os.path.isfile(f\"{self.save_to}/test_data.npy\"):\n",
    "                np.save(f\"{self.save_to}/test_data.npy\", data)\n",
    "            else:\n",
    "                print(f\"no delete old file\")\n",
    "            if not os.path.isfile(f\"{self.save_to}/test_label.npy\"):\n",
    "                np.save(f\"{self.save_to}/test_label.npy\", label)\n",
    "            else:\n",
    "                print(f\"no delete old file\")\n",
    "            print(f\"save to: {self.save_to}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process Class: benign\n",
      "Process Class: BF\n",
      "Process Class: Deauth\n",
      "Process Class: SAE_CV\n",
      "Process Class: UGD\n",
      "Process Class: SAE_AF\n",
      "Process Class: TSC\n",
      "Process Class: DG\n",
      "Process Class: KRACK_DG\n",
      "************************* Before Sampling *************************\n",
      "y: {0: 791020, 1: 43266, 2: 4405, 3: 968, 4: 1875, 5: 72723, 6: 655, 7: 9036, 8: 5864}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sun10/anaconda3/envs/IDS/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/sun10/anaconda3/envs/IDS/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/sun10/anaconda3/envs/IDS/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************* After OverSampling *************************\n",
      "y: {0: 791020, 1: 43266, 2: 4405, 3: 4000, 4: 4000, 5: 72723, 6: 4000, 7: 9036, 8: 5864}\n",
      "************************* Before Sampling *************************\n",
      "y: {0: 791020, 1: 43266, 2: 4405, 3: 4000, 4: 4000, 5: 72723, 6: 4000, 7: 9036, 8: 5864}\n",
      "************************* After UnderSampling *************************\n",
      "y: {0: 10000, 1: 10000, 2: 4405, 3: 4000, 4: 4000, 5: 10000, 6: 4000, 7: 9036, 8: 5864}\n",
      "save to: /home/sun10/IDS/upload/sampling\n"
     ]
    }
   ],
   "source": [
    "training_set_path = f\"{root_path}/data/train\"\n",
    "testing_set_path = f\"{root_path}/data/test\"\n",
    "\n",
    "# training set sampling\n",
    "sampling = Sampling(WIDS_class_list, data_path=training_set_path)\n",
    "\n",
    "OVERSAMPLING = dict({3: 4000, 4: 4000, 6: 4000})\n",
    "UNDERSAMPLING = dict({0: 10000, 1: 10000, 5: 10000})\n",
    "# load data\n",
    "data, label = sampling.load_data()\n",
    "if OVERSAMPLING != dict({}):\n",
    "    # oversampling\n",
    "    data, label = sampling.get_oversampling(\n",
    "        data, label, OVERSAMPLING, k=5, cluster_balance_threshold=0.0005\n",
    "    )\n",
    "if UNDERSAMPLING != dict({}):\n",
    "    # undersampling\n",
    "    data, label = sampling.get_undersampling(\n",
    "        data, label, under_strategy=UNDERSAMPLING\n",
    "    )\n",
    "# save\n",
    "sampling.save_np(data, label)\n",
    "del data, label, sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process Class: benign\n",
      "Process Class: BF\n",
      "Process Class: Deauth\n",
      "Process Class: SAE_CV\n",
      "Process Class: UGD\n",
      "Process Class: SAE_AF\n",
      "Process Class: TSC\n",
      "Process Class: DG\n",
      "Process Class: KRACK_DG\n",
      "************************* Before Sampling *************************\n",
      "y: {0: 237305, 1: 10817, 2: 1102, 3: 242, 4: 469, 5: 18181, 6: 164, 7: 2260, 8: 1466}\n",
      "************************* After UnderSampling *************************\n",
      "y: {0: 20000, 1: 10817, 2: 1102, 3: 242, 4: 469, 5: 18181, 6: 164, 7: 2260, 8: 1466}\n",
      "save to: /home/sun10/IDS/upload/sampling\n"
     ]
    }
   ],
   "source": [
    "# testing set sampling\n",
    "sampling = Sampling(WIDS_class_list, data_path=testing_set_path, test=True)\n",
    "\n",
    "OVERSAMPLING = dict({})\n",
    "UNDERSAMPLING = dict({0: 20000})\n",
    "# load data\n",
    "data, label = sampling.load_data()\n",
    "if OVERSAMPLING != dict({}):\n",
    "    # oversampling\n",
    "    data, label = sampling.get_oversampling(\n",
    "        data, label, OVERSAMPLING, k=5, cluster_balance_threshold=0.0005\n",
    "    )\n",
    "if UNDERSAMPLING != dict({}):\n",
    "    # undersampling\n",
    "    data, label = sampling.get_undersampling(\n",
    "        data, label, under_strategy=UNDERSAMPLING\n",
    "    )\n",
    "# save\n",
    "sampling.save_np(data, label)\n",
    "del data, label, sampling"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IDS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
